{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNr00sr/nKvC2VinVEn0hmn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dinusha-hue/Deep_Learning_Lab03/blob/main/DL_Lab_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "!unzip weights.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pcLgdi8oVQU",
        "outputId": "e7c82b26-7881-4831-9059-8a1ca3eb8613"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Archive:  weights.zip\n",
            "   creating: weights/\n",
            "  inflating: weights/bn1_b.csv       \n",
            "  inflating: weights/bn1_m.csv       \n",
            "  inflating: weights/bn1_v.csv       \n",
            "  inflating: weights/bn1_w.csv       \n",
            "  inflating: weights/bn2_b.csv       \n",
            "  inflating: weights/bn2_m.csv       \n",
            "  inflating: weights/bn2_v.csv       \n",
            "  inflating: weights/bn2_w.csv       \n",
            "  inflating: weights/bn3_b.csv       \n",
            "  inflating: weights/bn3_m.csv       \n",
            "  inflating: weights/bn3_v.csv       \n",
            "  inflating: weights/bn3_w.csv       \n",
            "  inflating: weights/conv1_b.csv     \n",
            "  inflating: weights/conv1_w.csv     \n",
            "  inflating: weights/conv2_b.csv     \n",
            "  inflating: weights/conv2_w.csv     \n",
            "  inflating: weights/conv3_b.csv     \n",
            "  inflating: weights/conv3_w.csv     \n",
            "  inflating: weights/dense_b.csv     \n",
            "  inflating: weights/dense_w.csv     \n",
            "  inflating: weights/inception_3a_1x1_bn_b.csv  \n",
            "  inflating: weights/inception_3a_1x1_bn_m.csv  \n",
            "  inflating: weights/inception_3a_1x1_bn_v.csv  \n",
            "  inflating: weights/inception_3a_1x1_bn_w.csv  \n",
            "  inflating: weights/inception_3a_1x1_conv_b.csv  \n",
            "  inflating: weights/inception_3a_1x1_conv_w.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn1_b.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn1_m.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn1_v.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn1_w.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn2_b.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn2_m.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn2_v.csv  \n",
            "  inflating: weights/inception_3a_3x3_bn2_w.csv  \n",
            "  inflating: weights/inception_3a_3x3_conv1_b.csv  \n",
            "  inflating: weights/inception_3a_3x3_conv1_w.csv  \n",
            "  inflating: weights/inception_3a_3x3_conv2_b.csv  \n",
            "  inflating: weights/inception_3a_3x3_conv2_w.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn1_b.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn1_m.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn1_v.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn1_w.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn2_b.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn2_m.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn2_v.csv  \n",
            "  inflating: weights/inception_3a_5x5_bn2_w.csv  \n",
            "  inflating: weights/inception_3a_5x5_conv1_b.csv  \n",
            "  inflating: weights/inception_3a_5x5_conv1_w.csv  \n",
            "  inflating: weights/inception_3a_5x5_conv2_b.csv  \n",
            "  inflating: weights/inception_3a_5x5_conv2_w.csv  \n",
            "  inflating: weights/inception_3a_pool_bn_b.csv  \n",
            "  inflating: weights/inception_3a_pool_bn_m.csv  \n",
            "  inflating: weights/inception_3a_pool_bn_v.csv  \n",
            "  inflating: weights/inception_3a_pool_bn_w.csv  \n",
            "  inflating: weights/inception_3a_pool_conv_b.csv  \n",
            "  inflating: weights/inception_3a_pool_conv_w.csv  \n",
            "  inflating: weights/inception_3b_1x1_bn_b.csv  \n",
            "  inflating: weights/inception_3b_1x1_bn_m.csv  \n",
            "  inflating: weights/inception_3b_1x1_bn_v.csv  \n",
            "  inflating: weights/inception_3b_1x1_bn_w.csv  \n",
            "  inflating: weights/inception_3b_1x1_conv_b.csv  \n",
            "  inflating: weights/inception_3b_1x1_conv_w.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn1_b.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn1_m.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn1_v.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn1_w.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn2_b.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn2_m.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn2_v.csv  \n",
            "  inflating: weights/inception_3b_3x3_bn2_w.csv  \n",
            "  inflating: weights/inception_3b_3x3_conv1_b.csv  \n",
            "  inflating: weights/inception_3b_3x3_conv1_w.csv  \n",
            "  inflating: weights/inception_3b_3x3_conv2_b.csv  \n",
            "  inflating: weights/inception_3b_3x3_conv2_w.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn1_b.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn1_m.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn1_v.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn1_w.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn2_b.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn2_m.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn2_v.csv  \n",
            "  inflating: weights/inception_3b_5x5_bn2_w.csv  \n",
            "  inflating: weights/inception_3b_5x5_conv1_b.csv  \n",
            "  inflating: weights/inception_3b_5x5_conv1_w.csv  \n",
            "  inflating: weights/inception_3b_5x5_conv2_b.csv  \n",
            "  inflating: weights/inception_3b_5x5_conv2_w.csv  \n",
            "  inflating: weights/inception_3b_pool_bn_b.csv  \n",
            "  inflating: weights/inception_3b_pool_bn_m.csv  \n",
            "  inflating: weights/inception_3b_pool_bn_v.csv  \n",
            "  inflating: weights/inception_3b_pool_bn_w.csv  \n",
            "  inflating: weights/inception_3b_pool_conv_b.csv  \n",
            "  inflating: weights/inception_3b_pool_conv_w.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn1_b.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn1_m.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn1_v.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn1_w.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn2_b.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn2_m.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn2_v.csv  \n",
            "  inflating: weights/inception_3c_3x3_bn2_w.csv  \n",
            "  inflating: weights/inception_3c_3x3_conv1_b.csv  \n",
            "  inflating: weights/inception_3c_3x3_conv1_w.csv  \n",
            "  inflating: weights/inception_3c_3x3_conv2_b.csv  \n",
            "  inflating: weights/inception_3c_3x3_conv2_w.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn1_b.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn1_m.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn1_v.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn1_w.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn2_b.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn2_m.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn2_v.csv  \n",
            "  inflating: weights/inception_3c_5x5_bn2_w.csv  \n",
            "  inflating: weights/inception_3c_5x5_conv1_b.csv  \n",
            "  inflating: weights/inception_3c_5x5_conv1_w.csv  \n",
            "  inflating: weights/inception_3c_5x5_conv2_b.csv  \n",
            "  inflating: weights/inception_3c_5x5_conv2_w.csv  \n",
            "  inflating: weights/inception_4a_1x1_bn_b.csv  \n",
            "  inflating: weights/inception_4a_1x1_bn_m.csv  \n",
            "  inflating: weights/inception_4a_1x1_bn_v.csv  \n",
            "  inflating: weights/inception_4a_1x1_bn_w.csv  \n",
            "  inflating: weights/inception_4a_1x1_conv_b.csv  \n",
            "  inflating: weights/inception_4a_1x1_conv_w.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn1_b.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn1_m.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn1_v.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn1_w.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn2_b.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn2_m.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn2_v.csv  \n",
            "  inflating: weights/inception_4a_3x3_bn2_w.csv  \n",
            "  inflating: weights/inception_4a_3x3_conv1_b.csv  \n",
            "  inflating: weights/inception_4a_3x3_conv1_w.csv  \n",
            "  inflating: weights/inception_4a_3x3_conv2_b.csv  \n",
            "  inflating: weights/inception_4a_3x3_conv2_w.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn1_b.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn1_m.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn1_v.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn1_w.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn2_b.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn2_m.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn2_v.csv  \n",
            "  inflating: weights/inception_4a_5x5_bn2_w.csv  \n",
            "  inflating: weights/inception_4a_5x5_conv1_b.csv  \n",
            "  inflating: weights/inception_4a_5x5_conv1_w.csv  \n",
            "  inflating: weights/inception_4a_5x5_conv2_b.csv  \n",
            "  inflating: weights/inception_4a_5x5_conv2_w.csv  \n",
            "  inflating: weights/inception_4a_pool_bn_b.csv  \n",
            "  inflating: weights/inception_4a_pool_bn_m.csv  \n",
            "  inflating: weights/inception_4a_pool_bn_v.csv  \n",
            "  inflating: weights/inception_4a_pool_bn_w.csv  \n",
            "  inflating: weights/inception_4a_pool_conv_b.csv  \n",
            "  inflating: weights/inception_4a_pool_conv_w.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn1_b.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn1_m.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn1_v.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn1_w.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn2_b.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn2_m.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn2_v.csv  \n",
            "  inflating: weights/inception_4e_3x3_bn2_w.csv  \n",
            "  inflating: weights/inception_4e_3x3_conv1_b.csv  \n",
            "  inflating: weights/inception_4e_3x3_conv1_w.csv  \n",
            "  inflating: weights/inception_4e_3x3_conv2_b.csv  \n",
            "  inflating: weights/inception_4e_3x3_conv2_w.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn1_b.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn1_m.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn1_v.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn1_w.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn2_b.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn2_m.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn2_v.csv  \n",
            "  inflating: weights/inception_4e_5x5_bn2_w.csv  \n",
            "  inflating: weights/inception_4e_5x5_conv1_b.csv  \n",
            "  inflating: weights/inception_4e_5x5_conv1_w.csv  \n",
            "  inflating: weights/inception_4e_5x5_conv2_b.csv  \n",
            "  inflating: weights/inception_4e_5x5_conv2_w.csv  \n",
            "  inflating: weights/inception_5a_1x1_bn_b.csv  \n",
            "  inflating: weights/inception_5a_1x1_bn_m.csv  \n",
            "  inflating: weights/inception_5a_1x1_bn_v.csv  \n",
            "  inflating: weights/inception_5a_1x1_bn_w.csv  \n",
            "  inflating: weights/inception_5a_1x1_conv_b.csv  \n",
            "  inflating: weights/inception_5a_1x1_conv_w.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn1_b.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn1_m.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn1_v.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn1_w.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn2_b.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn2_m.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn2_v.csv  \n",
            "  inflating: weights/inception_5a_3x3_bn2_w.csv  \n",
            "  inflating: weights/inception_5a_3x3_conv1_b.csv  \n",
            "  inflating: weights/inception_5a_3x3_conv1_w.csv  \n",
            "  inflating: weights/inception_5a_3x3_conv2_b.csv  \n",
            "  inflating: weights/inception_5a_3x3_conv2_m.csv  \n",
            "  inflating: weights/inception_5a_3x3_conv2_v.csv  \n",
            "  inflating: weights/inception_5a_3x3_conv2_w.csv  \n",
            "  inflating: weights/inception_5a_pool_bn_b.csv  \n",
            "  inflating: weights/inception_5a_pool_bn_m.csv  \n",
            "  inflating: weights/inception_5a_pool_bn_v.csv  \n",
            "  inflating: weights/inception_5a_pool_bn_w.csv  \n",
            "  inflating: weights/inception_5a_pool_conv_b.csv  \n",
            "  inflating: weights/inception_5a_pool_conv_w.csv  \n",
            "  inflating: weights/inception_5b_1x1_bn_b.csv  \n",
            "  inflating: weights/inception_5b_1x1_bn_m.csv  \n",
            "  inflating: weights/inception_5b_1x1_bn_v.csv  \n",
            "  inflating: weights/inception_5b_1x1_bn_w.csv  \n",
            "  inflating: weights/inception_5b_1x1_conv_b.csv  \n",
            "  inflating: weights/inception_5b_1x1_conv_w.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn1_b.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn1_m.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn1_v.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn1_w.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn2_b.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn2_m.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn2_v.csv  \n",
            "  inflating: weights/inception_5b_3x3_bn2_w.csv  \n",
            "  inflating: weights/inception_5b_3x3_conv1_b.csv  \n",
            "  inflating: weights/inception_5b_3x3_conv1_w.csv  \n",
            "  inflating: weights/inception_5b_3x3_conv2_b.csv  \n",
            "  inflating: weights/inception_5b_3x3_conv2_w.csv  \n",
            "  inflating: weights/inception_5b_pool_bn_b.csv  \n",
            "  inflating: weights/inception_5b_pool_bn_m.csv  \n",
            "  inflating: weights/inception_5b_pool_bn_v.csv  \n",
            "  inflating: weights/inception_5b_pool_bn_w.csv  \n",
            "  inflating: weights/inception_5b_pool_conv_b.csv  \n",
            "  inflating: weights/inception_5b_pool_conv_w.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install fr_utils"
      ],
      "metadata": {
        "id": "j2IfmQYTowaF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13HkR3SzpxLe",
        "outputId": "cb42bab6-0dc0-4af5-fc1e-bfb4a81d08e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "# from keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
        "# from keras.layers.merge import Concatenate\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.layers import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "# from keras.engine.topology import Layer\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from fr_utils import *\n",
        "from inception_blocks_v2 import *\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "\n",
        "# np.set_printoptions(threshold=np.nan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPHHj8Gqo9Px",
        "outputId": "a4882176-3e14-4134-fe27-fc4916dec842"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FRmodel = faceRecoModel(input_shape=(3, 96, 96))"
      ],
      "metadata": {
        "id": "RgnS-9SJrPel"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Params:\", FRmodel.count_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ab0nz7nv0xO",
        "outputId": "dc678ba9-c5a6-432f-bb33-ef22e7e56a03"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Params: 3743280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
        "    \"\"\"\n",
        "    Implementation of the triplet loss as defined by formula (3)\n",
        "\n",
        "    Arguments:\n",
        "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
        "    y_pred -- python list containing three objects:\n",
        "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
        "            positive -- the encodings for the positive images, of shape (None, 128)\n",
        "            negative -- the encodings for the negative images, of shape (None, 128)\n",
        "\n",
        "    Returns:\n",
        "    loss -- real number, value of the loss\n",
        "    \"\"\"\n",
        "\n",
        "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
        "\n",
        "    ### START CODE HERE ### (≈ 4 lines)\n",
        "    # Step 1: Compute the (encoding) distance between the anchor and the positive\n",
        "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)))\n",
        "    # Step 2: Compute the (encoding) distance between the anchor and the negative\n",
        "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)))\n",
        "    # Step 3: subtract the two previous distances and add alpha.\n",
        "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
        "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
        "    loss = tf.maximum(tf.reduce_mean(basic_loss), 0.0)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "lauGYFsav_M4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with tf.Session() as test:\n",
        "with tf.compat.v1.Session() as test:\n",
        "#    tf.set_random_seed(1)\n",
        "    tf.random.set_seed(1)\n",
        "    y_true = (None, None, None)\n",
        "    y_pred = (tf.random.normal([3, 128], mean=6, stddev=0.1, seed = 1),\n",
        "              tf.random.normal([3, 128], mean=1, stddev=1, seed = 1),\n",
        "              tf.random.normal([3, 128], mean=3, stddev=4, seed = 1))\n",
        "    loss = triplet_loss(y_true, y_pred)\n",
        "\n",
        "    print(\"loss = \" + str(loss.eval()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uKHBqcPwFy3",
        "outputId": "070c4772-7e46-4c5f-c0f2-46c5f09d9cf6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss = 350.02734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
        "load_weights_from_FaceNet(FRmodel)"
      ],
      "metadata": {
        "id": "SHd0lkkFwpQK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "database = {}\n",
        "database[\"danielle\"] = img_to_encoding(\"images/danielle.png\", FRmodel)\n",
        "database[\"younes\"] = img_to_encoding(\"images/younes.jpg\", FRmodel)\n",
        "database[\"tian\"] = img_to_encoding(\"images/tian.jpg\", FRmodel)\n",
        "database[\"andrew\"] = img_to_encoding(\"images/andrew.jpg\", FRmodel)\n",
        "database[\"kian\"] = img_to_encoding(\"images/kian.jpg\", FRmodel)\n",
        "database[\"dan\"] = img_to_encoding(\"images/dan.jpg\", FRmodel)\n",
        "database[\"sebastiano\"] = img_to_encoding(\"images/sebastiano.jpg\", FRmodel)\n",
        "database[\"bertrand\"] = img_to_encoding(\"images/bertrand.jpg\", FRmodel)\n",
        "database[\"kevin\"] = img_to_encoding(\"images/kevin.jpg\", FRmodel)\n",
        "database[\"felix\"] = img_to_encoding(\"images/felix.jpg\", FRmodel)\n",
        "database[\"benoit\"] = img_to_encoding(\"images/benoit.jpg\", FRmodel)\n",
        "database[\"arnaud\"] = img_to_encoding(\"images/arnaud.jpg\", FRmodel)\n",
        "database[\"rayn\"] = img_to_encoding(\"images/rayn.jpg\", FRmodel)"
      ],
      "metadata": {
        "id": "wprpcqUHxDKD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify(image_path, identity, database, model):\n",
        "\n",
        "    encoding = img_to_encoding(image_path, model)\n",
        "\n",
        "    dist = np.linalg.norm(encoding-database[identity])\n",
        "\n",
        "    if dist < 0.7:\n",
        "        print(\"It's \" + str(identity) + \", welcome home!\")\n",
        "        door_open = True\n",
        "    else:\n",
        "        print(\"It's not \" + str(identity) + \", please go away\")\n",
        "        door_open = False\n",
        "\n",
        "    return dist, door_open"
      ],
      "metadata": {
        "id": "eVrBfihZ751i"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verify(\"images/camera_0.jpg\", \"younes\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-EdWCrf8wLb",
        "outputId": "5a56249a-daed-4db2-b302-cd64d216126a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It's younes, welcome home!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6671399, True)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verify(\"images/camera_2.jpg\", \"kian\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2dAaTcn94rv",
        "outputId": "f5853f64-3079-49d2-8b28-b55499daaf45"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It's not kian, please go away\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8586889, False)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def who_is_it(image_path, database, model):\n",
        "\n",
        "    encoding = img_to_encoding(image_path, model)\n",
        "\n",
        "    min_dist = 100\n",
        "\n",
        "    output_data = {}\n",
        "\n",
        "    for (name, db_enc) in database.items():\n",
        "\n",
        "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database. (≈ 1 line)\n",
        "        dist = np.linalg.norm(encoding-db_enc)\n",
        "\n",
        "        #save L2 distances to interpret results\n",
        "        output_data[f'{name}'] = dist\n",
        "\n",
        "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n",
        "        if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            identity = name\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    if min_dist > 0.7:\n",
        "        print(\"Not in the database.\")\n",
        "    else:\n",
        "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
        "\n",
        "    return min_dist, identity, output_data"
      ],
      "metadata": {
        "id": "3A-jQ0F4-CuF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = who_is_it(\"images/camera_0.jpg\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS6Lz-dG-VWG",
        "outputId": "b67ffa6a-41fa-452e-f722-b9449c2eaa8d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it's younes, the distance is 0.6671399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-ocVtcz-ZZv",
        "outputId": "7124d9fa-4243-46c4-f048-6a4332b70e7f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'danielle': 1.2107376,\n",
              " 'younes': 0.6671399,\n",
              " 'tian': 1.221301,\n",
              " 'andrew': 0.9958904,\n",
              " 'kian': 0.8566895,\n",
              " 'dan': 0.7684447,\n",
              " 'sebastiano': 0.854448,\n",
              " 'bertrand': 0.7690577,\n",
              " 'kevin': 0.8715994,\n",
              " 'felix': 1.2063179,\n",
              " 'benoit': 0.8959498,\n",
              " 'arnaud': 0.8346038}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verify(\"images/camera_6.jpg\", \"rayn\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8xnn-d9CVfo",
        "outputId": "20a3ab43-89ef-4035-fd05-da4efc739e9d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It's not rayn, please go away\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9950889, False)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = who_is_it(\"images/camera_6.jpg\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4i7U1nHDrhq",
        "outputId": "0dee3194-535e-4f3c-82e6-ab56a1c40272"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not in the database.\n"
          ]
        }
      ]
    }
  ]
}